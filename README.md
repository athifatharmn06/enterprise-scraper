# ğŸ•·ï¸ Enterprise Price Intelligence Scraper

> A production-grade, distributed web scraping pipeline for real-time e-commerce price tracking. Built with Scrapy, Playwright, Celery, FastAPI, PostgreSQL, and Redis â€” fully containerised with Docker.

---

## ğŸ“¸ Screenshots

### API Documentation (Swagger UI)
![Swagger UI](docs/screenshots/1_swagger_ui.png)

### Live Products Endpoint â€” Enriched Data
![Products API](docs/screenshots/4_enriched_products.png)

### Categories Summary Endpoint
![Categories API](docs/screenshots/2_categories_api.png)

### pgAdmin â€” Database View (1,124 Products)
![pgAdmin](docs/screenshots/5_pgadmin.png)

> Shows `scraper_db` with all 4 tables (`alembic_version`, `price_history`, `products`, `retailers`) and the `products` table results â€” 1,124 total rows, query completed in 0.225s.

**Tables in `scraper_db`:**
| Table | Description |
|---|---|
| `retailers` | Source website metadata |
| `products` | 1,124+ unique products with enriched fields |
| `price_history` | Time-series price snapshots (grows every scrape) |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Compose                       â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   POST /scrape   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Celery Worker        â”‚   â”‚
â”‚  â”‚ FastAPI  â”‚                  â”‚  (Redis Broker)       â”‚   â”‚
â”‚  â”‚ :8000    â”‚â—€â”€â”€ JSON â”€â”€â”€â”€â”€â”€â”€â”€ â”‚                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚       â”‚                        â”‚  â”‚ Scrapy + Play-  â”‚  â”‚   â”‚
â”‚       â”‚                        â”‚  â”‚ wright Spiders  â”‚  â”‚   â”‚
â”‚       â–¼                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚PostgreSQLâ”‚â—€â”€â”€â”€â”€ SQLAlchemy  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”‚  :5432   â”‚      ORM + Alembic                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  Redis   â”‚  â† Task Queue / Result Backend               â”‚
â”‚  â”‚  :6379   â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Scraping** | [Scrapy 2.11](https://scrapy.org/) |
| **JS Rendering** | [Playwright](https://playwright.dev/) via `scrapy-playwright` |
| **Task Queue** | [Celery 5](https://docs.celeryq.dev/) + [Redis 7](https://redis.io/) |
| **API** | [FastAPI](https://fastapi.tiangolo.com/) + [Uvicorn](https://www.uvicorn.org/) |
| **Database** | [PostgreSQL 15](https://www.postgresql.org/) |
| **ORM** | [SQLAlchemy 2](https://www.sqlalchemy.org/) |
| **Migrations** | [Alembic](https://alembic.sqlalchemy.org/) |
| **Validation** | [Pydantic v2](https://docs.pydantic.dev/) |
| **Infrastructure** | [Docker](https://www.docker.com/) + [Docker Compose](https://docs.docker.com/compose/) |

---

## ğŸ—‚ï¸ Project Structure

```
enterprise-scraper/
â”œâ”€â”€ docker-compose.yml          # All 4 services: Postgres, Redis, API, Worker
â”œâ”€â”€ Dockerfile                  # Python app image (Playwright-ready)
â”œâ”€â”€ requirements.txt            # Pinned dependencies
â”œâ”€â”€ .env                        # Local environment variables
â”‚
â”œâ”€â”€ alembic/                    # Database migration scripts
â”‚   â”œâ”€â”€ env.py
â”‚   â””â”€â”€ versions/               # Auto-generated migration files
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ database.py             # SQLAlchemy engine & session factory
â”‚   â””â”€â”€ models.py               # ORM models: Retailer, Product, PriceHistory
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ settings.py             # Scrapy + Playwright config
â”‚   â”œâ”€â”€ items.py                # Scrapy Items + Pydantic validation schema
â”‚   â”œâ”€â”€ middlewares.py          # User-Agent rotation (anti-bot)
â”‚   â”œâ”€â”€ pipelines.py            # Validation pipeline + PostgreSQL write pipeline
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ ecommerce_spider.py # Playwright spider â†’ webscraper.io (electronics)
â”‚       â””â”€â”€ books_spider.py     # Fast HTTP spider â†’ books.toscrape.com (1,000 books)
â”‚
â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ celery_app.py           # Celery app config, Redis broker, Beat schedule
â”‚   â””â”€â”€ tasks.py                # 3 Celery tasks: ecommerce, books, full pipeline
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                 # FastAPI router with all endpoints
â”‚
â””â”€â”€ docs/
    â””â”€â”€ screenshots/            # Pipeline documentation screenshots
```

---

## ğŸš€ Quick Start

### Prerequisites

Make sure you have the following installed:
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) (includes Docker Compose)
- [Git](https://git-scm.com/)
- [pgAdmin](https://www.pgadmin.org/download/) *(optional, for visual DB browsing)*

---

### Step 1 â€” Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/enterprise-scraper.git
cd enterprise-scraper
```

---

### Step 2 â€” Configure Environment Variables

The `.env` file is already set up with local defaults. No changes needed for local development:

```env
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=scraper_pass
POSTGRES_DB=scraper_db
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
REDIS_URL=redis://redis:6379/0
```

---

### Step 3 â€” Build and Start All Services

```bash
docker compose up -d --build
```

This starts **4 containers**:

| Container | Service | Port |
|---|---|---|
| `enterprise_scraper_db` | PostgreSQL 15 | `5432` |
| `enterprise_scraper_redis` | Redis 7 | `6379` |
| `enterprise_scraper_api` | FastAPI | `8000` |
| `enterprise_scraper_worker` | Celery Worker | â€” |

Verify all are running:
```bash
docker ps
```

---

### Step 4 â€” Apply Database Migrations

Run once after first startup to create all tables:

```bash
# Generate migration file from SQLAlchemy models
docker exec enterprise_scraper_api bash -c "alembic revision --autogenerate -m 'initial_schema'"

# Apply migrations to create tables in PostgreSQL
docker exec enterprise_scraper_api bash -c "alembic upgrade head"
```

---

### Step 5 â€” Trigger Your First Scrape

**Option A â€” Full pipeline (all spiders, recommended):**
```bash
curl -X POST http://localhost:8000/scrape/trigger/all
```

**Option B â€” Electronics only (webscraper.io):**
```bash
curl -X POST http://localhost:8000/scrape/trigger
```

**Option C â€” Books only (books.toscrape.com, 1,000 books):**
```bash
curl -X POST http://localhost:8000/scrape/trigger/books
```

> The scrape runs **asynchronously**. The API returns immediately with a `task_id`. The Celery worker processes the job in the background (usually 15â€“60 seconds depending on network).

---

### Step 6 â€” View the Results

Open your browser and visit:

| URL | Description |
|---|---|
| [`http://localhost:8000/docs`](http://localhost:8000/docs) | **Swagger UI** â€” Interactive API docs |
| [`http://localhost:8000/api/v1/products`](http://localhost:8000/api/v1/products) | All scraped products (JSON) |
| [`http://localhost:8000/api/v1/categories`](http://localhost:8000/api/v1/categories) | Product count by category |
| [`http://localhost:8000/api/v1/products/1/prices`](http://localhost:8000/api/v1/products/1/prices) | Price history for product ID 1 |

---

## ğŸ˜ Connecting pgAdmin (Visual Database Browser)

1. Install and open [pgAdmin](https://www.pgadmin.org/download/)
2. Right-click **Servers** â†’ **Register** â†’ **Server...**
3. Fill in the connection details:

| Field | Value |
|---|---|
| **Name** | `Enterprise Scraper` *(anything)* |
| **Host** | `localhost` |
| **Port** | `5432` |
| **Username** | `scraper_user` |
| **Password** | `scraper_pass` |
| **Database** | `scraper_db` |

4. Expand: `Enterprise Scraper â†’ Databases â†’ scraper_db â†’ Schemas â†’ public â†’ Tables`
5. Right-click any table â†’ **View/Edit Data â†’ All Rows**

**Useful SQL queries to run in pgAdmin:**
```sql
-- Total products per category
SELECT category, COUNT(*) FROM products GROUP BY category ORDER BY COUNT(*) DESC;

-- Latest price for each product
SELECT p.name, p.category, ph.price, ph.currency, ph.scraped_at
FROM products p
JOIN price_history ph ON ph.product_id = p.id
WHERE ph.scraped_at = (
    SELECT MAX(scraped_at) FROM price_history WHERE product_id = p.id
)
ORDER BY p.category, p.name
LIMIT 50;

-- Time-series: how a product's price changes over scrapes
SELECT ph.price, ph.scraped_at FROM price_history ph WHERE product_id = 1 ORDER BY ph.scraped_at;
```

---

## ğŸ“¡ API Reference

### Scraping Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/scrape/trigger` | Run electronics spider (webscraper.io) |
| `POST` | `/scrape/trigger/books` | Run books spider (1,000 books) |
| `POST` | `/scrape/trigger/all` | Run **all spiders** â€” maximum data |

### Data Endpoints

| Method | Endpoint | Query Params | Description |
|---|---|---|---|
| `GET` | `/api/v1/products` | `?skip=0&limit=100&category=laptops` | List all products with enriched metadata |
| `GET` | `/api/v1/categories` | â€” | Product counts per category |
| `GET` | `/api/v1/products/{id}/prices` | â€” | Full price history for one product |

### Sample Response â€” `/api/v1/products`
```json
[
  {
    "id": 1,
    "name": "Lenovo IdeaPad 320-15ABR",
    "sku": "88",
    "category": "laptops",
    "brand": null,
    "description": "Lenovo IdeaPad 320, 15.6\" HD, A9-9420 3GHz...",
    "image_url": "https://webscraper.io/.../88.jpg",
    "rating": 3.0,
    "review_count": null,
    "url": "https://webscraper.io/.../product/88",
    "retailer": "WebScraper Test Site"
  }
]
```

---

## ğŸ•·ï¸ Spiders

### `ecommerce` â€” Electronics (webscraper.io)
- **Rendering:** Playwright (handles JavaScript SPAs)
- **Categories:** Laptops, Tablets, Phones, Touch Phones
- **Products:** ~147 unique items
- **Trigger:** `POST /scrape/trigger`

### `books` â€” Books to Scrape (books.toscrape.com)
- **Rendering:** Standard HTTP (high-performance, no JS needed)
- **Categories:** 50 genres (Fiction, Mystery, Science, etc.)
- **Products:** 1,000 unique books
- **Trigger:** `POST /scrape/trigger/books`

---

## ğŸ›¡ï¸ Anti-Bot Measures

- **User-Agent Rotation** â€” Random browser User-Agent on every request
- **Playwright Rendering** â€” Full Chrome browser for JS-heavy pages
- **Rate Limiting** â€” Configurable download delay between requests
- **Proxy Scaffolding** â€” `ProxyRotatorMiddleware` ready for enterprise proxy integration

---

## ğŸ“ˆ Data Growth Pattern

Every time you call `/scrape/trigger/all`, the system:
1. Re-scrapes **all** product pages
2. **Upserts** product metadata (price, rating, description)
3. **Appends** a new `price_history` row per product

This creates a **time-series price intelligence dataset**. Run it daily and you'll track price changes over time across 1,000+ products.

```bash
# Example: run full scrape 5 times to build history
for i in 1 2 3 4 5; do
  curl -X POST http://localhost:8000/scrape/trigger/all
  sleep 30
done
```

---

## ğŸ”„ Stopping and Restarting

```bash
# Stop all services (keeps data)
docker compose down

# Restart everything
docker compose up -d

# Full reset (DELETES all data)
docker compose down -v
```

---

## ğŸ§° Useful Docker Commands

```bash
# View live logs from the Celery worker
docker logs enterprise_scraper_worker -f

# View API logs
docker logs enterprise_scraper_api -f

# Open a database shell
docker exec -it enterprise_scraper_db psql -U scraper_user -d scraper_db

# Run a spider directly (for testing)
docker exec enterprise_scraper_worker bash -c "python -m scrapy crawl books"
```

---

## ğŸ—„ï¸ Database Schema

```
retailers
â”œâ”€â”€ id (PK)
â”œâ”€â”€ name
â”œâ”€â”€ domain (unique)
â””â”€â”€ created_at

products
â”œâ”€â”€ id (PK)
â”œâ”€â”€ retailer_id (FK â†’ retailers)
â”œâ”€â”€ name
â”œâ”€â”€ url (unique)
â”œâ”€â”€ sku
â”œâ”€â”€ category          â† indexed for fast filtering
â”œâ”€â”€ description
â”œâ”€â”€ image_url
â”œâ”€â”€ rating
â”œâ”€â”€ review_count
â”œâ”€â”€ brand
â”œâ”€â”€ created_at
â””â”€â”€ updated_at

price_history
â”œâ”€â”€ id (PK, BigInteger)
â”œâ”€â”€ product_id (FK â†’ products)
â”œâ”€â”€ price
â”œâ”€â”€ currency
â”œâ”€â”€ in_stock
â””â”€â”€ scraped_at        â† indexed for time-series queries
```

---

## ğŸ“‹ License

MIT License â€” free to use, modify and distribute.
